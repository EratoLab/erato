#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "pydantic>=2.0.0",
#   "pyyaml>=6.0",
# ]
# ///

import argparse
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional
from pydantic import BaseModel, Field

# Import shared k3d utilities
from k3d_common import get_helm_scenario_args, validate_scenario, VALID_SCENARIOS


class ScriptConfig(BaseModel):
    """Configuration for the setup script."""
    wait_flag: str = ""
    erato_image_repository: str = "harbor.imassage.me/erato/app"
    erato_image_tag: Optional[str] = None
    build_local: bool = False
    helm_set_args: list[str] = Field(default_factory=list)
    chart_dep_update: bool = False
    with_telepresence: bool = False
    file_storage_config_source: Optional[str] = None
    test_scenario: str = "basic"


class ClusterConfig(BaseModel):
    """Configuration for the k3d cluster."""
    cluster_name: str = "erato-dev"
    erato_chart_path: Path = Path("./charts/erato")
    chart_path: Path = Path("./k3d/erato-local")
    app_host: str = "app.erato.internal"
    dex_host: str = "dex.erato.internal"


class ClusterStatus(BaseModel):
    """Status of a k3d cluster."""
    exists: bool
    running: bool


class BuildConfig(BaseModel):
    """Configuration for local image build."""
    project_root: Path
    build_tag: str
    local_registry: str = "harbor.imassage.me"
    frontend_image_name: str = "erato/frontend"
    repo_base: str
    tag_base: str
    backend_image_repo: str
    base_backend_image: str
    combined_image_path: str
    local_frontend_image: str
    local_combined_image: str
    backend_image_for_build_arg: str


def run_command(cmd: list[str], check: bool = True, capture_output: bool = False, 
                shell: bool = False, input: Optional[str] = None, cwd: Optional[Path] = None) -> subprocess.CompletedProcess:
    """Run a shell command with error handling."""
    try:
        if shell:
            cmd_str = " ".join(cmd) if isinstance(cmd, list) else cmd
            result = subprocess.run(cmd_str, shell=True, check=check, 
                                  capture_output=capture_output, text=True, 
                                  input=input, cwd=cwd)
        else:
            result = subprocess.run(cmd, check=check, capture_output=capture_output, 
                                  text=True, input=input, cwd=cwd)
        return result
    except subprocess.CalledProcessError as e:
        print(f"Error running command: {' '.join(cmd)}", file=sys.stderr)
        if e.stderr:
            print(f"Stderr: {e.stderr}", file=sys.stderr)
        if check:
            sys.exit(1)
        raise
    except FileNotFoundError:
        print(f"Error: Command not found: {cmd[0]}", file=sys.stderr)
        sys.exit(1)


def check_prerequisites():
    """Check if required commands are available."""
    required_commands = ["k3d", "kubectl", "helm"]
    for cmd in required_commands:
        result = run_command(["which", cmd], check=False, capture_output=True)
        if result.returncode != 0:
            print(f"{cmd} is required but not installed. Aborting.", file=sys.stderr)
            sys.exit(1)


def check_e2e_secrets() -> bool:
    """Check if E2E secrets are configured.

    Returns:
        True if secrets are configured, False otherwise
    """
    script_dir = Path(__file__).resolve().parent
    try:
        result = run_command(
            [str(script_dir / "setup-e2e-secrets"), "check"],
            check=False,
            capture_output=True
        )
        return result.returncode == 0
    except Exception:
        return False


def get_cluster_status(cluster_name: str) -> ClusterStatus:
    """Check if a k3d cluster exists and if it's running."""
    import json
    
    result = run_command(["k3d", "cluster", "list", "--output", "json"], capture_output=True)
    
    try:
        clusters = json.loads(result.stdout)
        for cluster in clusters:
            if cluster.get("name") == cluster_name:
                # Check if all expected nodes are running
                servers_running = cluster.get("serversRunning", 0)
                servers_count = cluster.get("serversCount", 0)
                agents_running = cluster.get("agentsRunning", 0)
                agents_count = cluster.get("agentsCount", 0)
                
                # Cluster is running if all servers and agents are running
                is_running = (servers_running == servers_count and 
                            agents_running == agents_count and 
                            servers_count > 0)
                
                return ClusterStatus(exists=True, running=is_running)
        
        return ClusterStatus(exists=False, running=False)
    except json.JSONDecodeError:
        # Fallback if JSON parsing fails
        print("Warning: Could not parse k3d cluster list output", file=sys.stderr)
        return ClusterStatus(exists=False, running=False)


def create_or_use_cluster(cluster_config: ClusterConfig):
    """Create k3d cluster if it doesn't exist, start it if stopped, otherwise use existing."""
    status = get_cluster_status(cluster_config.cluster_name)
    
    if not status.exists:
        print(f"Creating k3d cluster '{cluster_config.cluster_name}'...")
        run_command([
            "k3d", "cluster", "create",
            cluster_config.cluster_name,
            "--config", "k3d/cluster-config.yaml"
        ])
    elif not status.running:
        print(f"Starting existing cluster '{cluster_config.cluster_name}'...")
        run_command(["k3d", "cluster", "start", cluster_config.cluster_name])
    else:
        print(f"Using existing cluster '{cluster_config.cluster_name}'...")


def wait_for_cluster():
    """Wait for cluster nodes to be ready."""
    print("Waiting for cluster to be ready...")
    
    # Retry a few times in case the API server isn't immediately available
    max_retries = 5
    for attempt in range(1, max_retries + 1):
        result = run_command([
            "kubectl", "wait", "--for=condition=Ready",
            "nodes", "--all", "--timeout=60s"
        ], check=False, capture_output=True)
        
        if result.returncode == 0:
            return
        
        if attempt < max_retries:
            print(f"Attempt {attempt}/{max_retries}: Cluster not ready yet, waiting 5 seconds...")
            time.sleep(5)
        else:
            print("Error: Cluster did not become ready in time", file=sys.stderr)
            print(f"Last error: {result.stderr}", file=sys.stderr)
            sys.exit(1)


def wait_for_coredns():
    """Wait for CoreDNS to be ready."""
    print("Waiting for CoreDNS to be ready...")
    run_command([
        "kubectl", "-n", "kube-system", "wait",
        "--for=condition=Available", "deployment/coredns",
        "--timeout=60s"
    ])


def add_helm_repos():
    """Add required Helm repositories."""
    repos = [
        ("bitnami", "https://charts.bitnami.com/bitnami"),
        ("cnpg", "https://cloudnative-pg.github.io/charts"),
        ("datawire", "https://app.getambassador.io"),
        ("seaweedfs", "https://seaweedfs.github.io/seaweedfs/helm"),
    ]
    for name, url in repos:
        run_command(["helm", "repo", "add", name, url], check=False, capture_output=True)


def update_chart_dependencies(config: ScriptConfig, cluster_config: ClusterConfig):
    """Update Helm chart dependencies if requested."""
    if config.chart_dep_update:
        print("Updating chart dependencies...")
        # Update dependencies for both charts.
        # We don't run 'helm repo update' separately as 'helm dependency update'
        # already does this. We use --skip-refresh for the second chart to avoid
        # redundant repository updates.
        run_command(["helm", "dependency", "update", str(cluster_config.erato_chart_path)])
        run_command(["helm", "dependency", "update", "--skip-refresh", str(cluster_config.chart_path)])


def configure_helm_args(config: ScriptConfig) -> list[str]:
    """Configure Helm arguments based on script config."""
    helm_args = config.helm_set_args.copy()
    
    if config.erato_image_repository:
        helm_args.extend(["--set", f"erato.backend.image.repository={config.erato_image_repository}"])
    
    if config.erato_image_tag:
        helm_args.extend(["--set", f"erato.backend.image.tag={config.erato_image_tag}"])
    
    return helm_args


def perform_local_build(config: ScriptConfig) -> list[str]:
    """Perform local build and return Helm arguments."""
    project_root = Path(__file__).resolve().parent.parent.parent
    build_tag = str(int(datetime.now().timestamp()))
    
    repo_base = config.erato_image_repository or "harbor.imassage.me/erato/app"
    tag_base = config.erato_image_tag or "latest"
    
    backend_image_repo = repo_base.replace("/app", "/backend")
    base_backend_image = f"{backend_image_repo}:{tag_base}"
    print(f"Using base backend image: {base_backend_image}")
    
    combined_image_path = "/".join(repo_base.split("/")[1:])
    
    build_config = BuildConfig(
        project_root=project_root,
        build_tag=build_tag,
        repo_base=repo_base,
        tag_base=tag_base,
        backend_image_repo=backend_image_repo,
        base_backend_image=base_backend_image,
        combined_image_path=combined_image_path,
        local_frontend_image=f"harbor.imassage.me/erato/frontend:{build_tag}",
        local_combined_image=f"k3d-registry.localhost:5000/{combined_image_path}:{build_tag}",
        backend_image_for_build_arg="/".join(base_backend_image.split("/")[1:])
    )
    
    print(f"Building frontend image: {build_config.local_frontend_image}")
    run_command([
        "docker", "build",
        "-t", build_config.local_frontend_image,
        "-f", str(build_config.project_root / "frontend" / "Dockerfile"),
        str(build_config.project_root / "frontend"),
        "--platform=linux/amd64"
    ])
    run_command(["docker", "push", build_config.local_frontend_image])
    
    print(f"Building combined image: {build_config.local_combined_image}")
    run_command([
        "docker", "build",
        "--build-arg", f"REGISTRY={build_config.local_registry}",
        "--build-arg", f"FRONTEND_IMAGE={build_config.frontend_image_name}:{build_config.build_tag}",
        "--build-arg", f"BACKEND_IMAGE={build_config.backend_image_for_build_arg}",
        "-t", build_config.local_combined_image,
        "-f", str(build_config.project_root / "Dockerfile.combined"),
        str(build_config.project_root),
        "--platform=linux/amd64"
    ])
    run_command(["docker", "push", build_config.local_combined_image])
    
    return [
        "--set", f"erato.backend.image.repository=k3d-registry.localhost:5000/{build_config.combined_image_path}",
        "--set", f"erato.backend.image.tag={build_config.build_tag}"
    ]


def handle_file_storage_config(config: ScriptConfig) -> list[str]:
    """Handle file storage configuration source."""
    if not config.file_storage_config_source:
        return []
    
    return ["--set", f"fileStorageConfig.sourceFile={config.file_storage_config_source}"]


def helm_release_exists(namespace: str, release_name: str) -> bool:
    """Check if a Helm release exists."""
    result = run_command(
        ["helm", "list", "-n", namespace],
        capture_output=True,
        check=False
    )
    for line in result.stdout.splitlines():
        if line.startswith(release_name):
            return True
    return False


def install_nginx_ingress():
    """Install nginx ingress controller."""
    print("Installing nginx ingress controller...")
    if not helm_release_exists("ingress-nginx", "ingress-nginx"):
        print("Installing nginx ingress controller...")
        run_command([
            "helm", "upgrade", "--install", "ingress-nginx", "ingress-nginx",
            "--repo", "https://kubernetes.github.io/ingress-nginx",
            "--namespace", "ingress-nginx", "--create-namespace"
        ])
    else:
        print("nginx ingress controller already installed, skipping...")


def install_telepresence(config: ScriptConfig):
    """Install Telepresence if requested."""
    if config.with_telepresence:
        if not helm_release_exists("ambassador", "traffic-manager"):
            print("Installing Telepresence...")
            run_command(["telepresence", "helm", "install"])
        else:
            print("Telepresence already installed, skipping...")
    else:
        print("Skipping Telepresence installation (use --with-telepresence to enable)")


def install_cnpg():
    """Install CNPG operator."""
    if not helm_release_exists("cnpg-system", "cnpg"):
        print("Installing CNPG operator...")
        run_command([
            "helm", "upgrade", "--install", "cnpg",
            "--namespace", "cnpg-system",
            "--create-namespace",
            "cnpg/cloudnative-pg"
        ])
    else:
        print("CNPG operator already installed, skipping...")


def install_reloader():
    """Install Reloader for ConfigMap auto-reloading."""
    # Check if reloader is already installed by checking for the deployment
    # Reloader is typically installed in the default namespace
    result = run_command([
        "kubectl", "get", "deployment",
        "reloader-reloader",
        "-n", "default"
    ], capture_output=True, check=False)
    
    if result.returncode == 0:
        print("Reloader already installed, skipping...")
        return
    
    print("Installing Reloader for ConfigMap auto-reloading...")
    run_command([
        "kubectl", "apply", "-f",
        "https://raw.githubusercontent.com/stakater/Reloader/master/deployments/kubernetes/reloader.yaml"
    ])


def wait_for_webhook(namespace: str, service_name: str, webhook_name: str, max_attempts: int = 60):
    """Wait for a webhook service to be ready."""
    print(f"Waiting for {webhook_name} to be ready...")
    
    for attempt in range(1, max_attempts + 1):
        result = run_command([
            "kubectl", "get", "endpoints",
            "-n", namespace, service_name,
            "-o", "jsonpath={.subsets[*].addresses[*].ip}"
        ], capture_output=True, check=False)
        
        if result.returncode == 0 and result.stdout.strip():
            print("Webhook is ready!")
            return
        
        print(f"Attempt {attempt}/{max_attempts}: Webhook not ready, waiting 5 seconds...")
        time.sleep(5)
    
    print("Timeout: Webhook did not become ready", file=sys.stderr)
    sys.exit(1)


def install_erato_chart(config: ScriptConfig, cluster_config: ClusterConfig, helm_args: list[str]):
    """Install/upgrade Erato local development chart."""
    print("Installing/upgrading Erato local development chart...")
    
    cmd = [
        "helm", "upgrade", "--install", "erato-local",
        str(cluster_config.chart_path),
        "--namespace", "erato-local-ns", "--create-namespace"
    ]
    
    if config.wait_flag:
        cmd.append(config.wait_flag)
    
    cmd.extend(helm_args)
    
    print(f"Running: {' '.join(cmd)}")
    run_command(cmd)


def ensure_hosts_entries(cluster_config: ClusterConfig):
    """Ensure local DNS entries exist in /etc/hosts."""
    hosts = [
        cluster_config.app_host,
        cluster_config.dex_host,
        "k3d-registry.localhost"
    ]
    
    try:
        with open("/etc/hosts", "r") as f:
            hosts_content = f.read()
    except PermissionError:
        print("Warning: Cannot read /etc/hosts. You may need to add entries manually.", file=sys.stderr)
        return
    
    for host in hosts:
        if host not in hosts_content:
            print(f"Adding {host} to /etc/hosts...")
            entry = f"127.0.0.1 {host}\n"
            try:
                result = run_command(
                    ["sudo", "tee", "-a", "/etc/hosts"],
                    capture_output=True,
                    input=entry
                )
            except Exception as e:
                print(f"Warning: Could not add {host} to /etc/hosts: {e}", file=sys.stderr)
                print(f"Please manually add: 127.0.0.1 {host}", file=sys.stderr)


def print_completion_message(cluster_config: ClusterConfig):
    """Print completion message with access information."""
    print()
    print("Setup complete! Your development environment is ready.")
    print(f"Access the application at: https://{cluster_config.app_host}")
    print(f"Access Dex at: https://{cluster_config.dex_host}")
    print()
    print("Default login credentials:")
    print("  Username: admin@example.com")
    print("  Password: admin")


def main():
    parser = argparse.ArgumentParser(
        description="Setup local k3d development environment for Erato",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--wait",
        action="store_true",
        help="Wait for resources to be ready during Helm install"
    )
    parser.add_argument(
        "--build-local",
        action="store_true",
        help="Build images locally instead of pulling from registry"
    )
    parser.add_argument(
        "--erato-image-repository",
        default="harbor.imassage.me/erato/app",
        help="Erato image repository (default: harbor.imassage.me/erato/app)"
    )
    parser.add_argument(
        "--erato-image-tag",
        default=None,
        help="Erato image tag (if not specified, uses chart default)"
    )
    parser.add_argument(
        "--chart-dep-update",
        action="store_true",
        help="Update Helm chart dependencies"
    )
    parser.add_argument(
        "--with-telepresence",
        action="store_true",
        help="Install Telepresence for local development"
    )
    parser.add_argument(
        "--file-storage-config",
        dest="file_storage_config_source",
        help="Source file for file storage config (e.g., 'config/erato.file-storage.toml')"
    )
    parser.add_argument(
        "--scenario",
        default="basic",
        choices=VALID_SCENARIOS,
        help=f"Test scenario to deploy (default: basic). Choices: {', '.join(VALID_SCENARIOS)}"
    )
    
    args = parser.parse_args()
    
    # Validate scenario
    if not validate_scenario(args.scenario):
        print(f"Error: Invalid scenario '{args.scenario}'", file=sys.stderr)
        print(f"Valid scenarios: {', '.join(VALID_SCENARIOS)}", file=sys.stderr)
        sys.exit(1)
    
    # Build configuration
    config = ScriptConfig(
        wait_flag="--wait" if args.wait else "",
        erato_image_repository=args.erato_image_repository,
        erato_image_tag=args.erato_image_tag,
        build_local=args.build_local,
        chart_dep_update=args.chart_dep_update,
        with_telepresence=args.with_telepresence,
        file_storage_config_source=args.file_storage_config_source,
        test_scenario=args.scenario
    )
    
    cluster_config = ClusterConfig()
    
    # Change to infrastructure directory
    script_dir = Path(__file__).resolve().parent
    infrastructure_dir = script_dir.parent
    
    try:
        import os
        os.chdir(infrastructure_dir)
    except Exception as e:
        print(f"Error: Could not change to infrastructure directory: {e}", file=sys.stderr)
        sys.exit(1)
    
    # Execute setup steps
    check_prerequisites()

    # Check E2E secrets configuration
    if not check_e2e_secrets():
        print("⚠️  WARNING: E2E secrets not configured.")
        print("   Run: ./scripts/setup-e2e-secrets apply")
        print("   See: infrastructure/k3d/erato-local/config/e2e-secrets.template.toml")
        print()

    create_or_use_cluster(cluster_config)
    
    # Switch kubectl context
    run_command(["kubectl", "config", "use-context", f"k3d-{cluster_config.cluster_name}"])
    
    wait_for_cluster()
    wait_for_coredns()
    
    add_helm_repos()
    update_chart_dependencies(config, cluster_config)
    
    # Configure Helm arguments
    helm_args = configure_helm_args(config)
    
    # Handle local build if requested
    if config.build_local:
        print("Performing local build...")
        build_helm_args = perform_local_build(config)
        helm_args = build_helm_args  # Override with build-specific args

    # Handle E2E scenario secrets
    # Check if any scenario secret files exist
    secrets_exist = any(
        (cluster_config.chart_path / f"config/erato.scenario-{scenario}.auto.toml").exists()
        for scenario in VALID_SCENARIOS
    )

    if secrets_exist:
        import json
        helm_args.extend(["--set", "scenarioSecretsConfig.enabled=true"])
        helm_args.extend(["--set", f"scenarioSecretsConfig.scenario={config.test_scenario}"])
        # Add E2E scenario data server ingress path for test credential access
        extra_ingress_paths = [
            {
                "path": "/e2e-scenario-data",
                "pathType": "Prefix",
                "backend": {
                    "service": {
                        "name": "{{ .Release.Name }}-e2e-scenario-data-server",
                        "port": {
                            "name": "http"
                        }
                    }
                }
            }
        ]
        helm_args.extend(["--set-json", f"erato.ingress.extraPaths={json.dumps(extra_ingress_paths)}"])
        # Build the complete extraConfigFiles array with the scenario secrets entry
        # Replace underscores with hyphens for K8s RFC 1123 compliance
        scenario_k8s_name = config.test_scenario.replace("_", "-")
        secret_name = f"erato-local-erato-scenario-{scenario_k8s_name}-secrets"
        extra_config_files = [
            {
                "name": "file-storage",
                "configMapName": "erato-local-erato-file-storage-config",
                "configMapKey": "erato.file-storage.toml"
            },
            {
                "name": "test-scenario",
                "configMapName": "erato-local-erato-test-scenario-config",
                "configMapKey": "erato.scenario.toml"
            },
            {
                "name": "scenario-secrets",
                "secretName": secret_name,
                "secretKey": "erato.scenario.auto.toml"
            },
            {
                "name": "frontend-env",
                "inlineContent": "[frontend.additional_environment]\nFOO_BAR = \"true\""
            }
        ]
        helm_args.extend(["--set-json", f"erato.backend.extraConfigFiles={json.dumps(extra_config_files)}"])

    # Handle OAuth2-Proxy configuration for entra_id scenario
    oauth2_values_file = None
    if secrets_exist and config.test_scenario == "entra_id":
        secrets_file = cluster_config.chart_path / "config" / "e2e-secrets.toml"

        if secrets_file.exists():
            try:
                import tomllib
                import yaml
                import tempfile

                # Load secrets
                with open(secrets_file, "rb") as f:
                    secrets = tomllib.load(f)

                # Read base oauth2-proxy config from values.yaml
                values_file = cluster_config.chart_path / "values.yaml"
                with open(values_file, "r") as f:
                    values_data = yaml.safe_load(f)
                base_config = values_data.get("erato", {}).get("oauth2Proxy", {}).get("config", "")

                # Parse base config into a dictionary
                config_dict = {}
                for line in base_config.split('\n'):
                    line = line.strip()
                    if '=' in line and not line.startswith('#'):
                        key, value = line.split('=', 1)
                        config_dict[key.strip()] = value.strip()

                # Apply Entra ID overrides
                config_dict['provider'] = '"entra-id"'
                config_dict['oidc_issuer_url'] = f'"https://login.microsoftonline.com/{secrets["entra_id_tenant_id"]}/v2.0"'
                config_dict['client_id'] = f'"{secrets["entra_id_client_id"]}"'
                config_dict['client_secret'] = f'"{secrets["entra_id_client_secret"]}"'
                config_dict['skip_oidc_discovery'] = 'false'

                # Remove Dex-specific fields that Entra ID doesn't need
                config_dict.pop('login_url', None)
                config_dict.pop('redeem_url', None)
                config_dict.pop('oidc_jwks_url', None)
                config_dict.pop('insecure_oidc_skip_issuer_verification', None)

                # Reconstruct config
                merged_lines = [f"{k} = {v}" for k, v in config_dict.items()]
                merged_config = '\n'.join(merged_lines)

                # Write to a temporary values file
                # Using a values file preserves multiline strings properly
                oauth2_values = {
                    "erato": {
                        "oauth2Proxy": {
                            "config": merged_config
                        }
                    }
                }

                # Create temp file that won't be auto-deleted
                fd, oauth2_values_file = tempfile.mkstemp(suffix='.yaml', text=True)
                with os.fdopen(fd, 'w') as f:
                    yaml.dump(oauth2_values, f)

                helm_args.extend(["-f", oauth2_values_file])

                print(f"✓ Configured oauth2-proxy for Entra ID authentication")

            except Exception as e:
                print(f"Warning: Could not merge oauth2-proxy config: {e}", file=sys.stderr)

    # Handle file storage config
    file_storage_args = handle_file_storage_config(config)
    helm_args.extend(file_storage_args)
    
    # Handle test scenario
    scenario_args = get_helm_scenario_args(config.test_scenario)
    helm_args.extend(scenario_args)
    
    # Install components
    install_nginx_ingress()
    install_telepresence(config)
    install_cnpg()
    install_reloader()
    
    # Wait for webhooks
    wait_for_webhook("ingress-nginx", "ingress-nginx-controller-admission", "NGINX admission webhook")
    wait_for_webhook("cnpg-system", "cnpg-webhook-service", "CNPG webhook")
    
    # Install Erato chart
    install_erato_chart(config, cluster_config, helm_args)

    # Clean up temporary values file if it was created
    if oauth2_values_file and os.path.exists(oauth2_values_file):
        try:
            os.unlink(oauth2_values_file)
        except Exception:
            pass  # Ignore cleanup errors

    # Ensure DNS entries
    ensure_hosts_entries(cluster_config)
    
    # Print completion message
    print_completion_message(cluster_config)


if __name__ == "__main__":
    main()

